import os
import torch
from transformers import (
    AutoModelForSequenceClassification,
    AutoTokenizer
)
from uuid import uuid4
import time
import random
from typing import Any
from swarmclone.module_bootstrap import *

@dataclass
class LLMConfig(ModuleConfig):
    chat_maxsize: int = field(default=20, metadata={
        "required": False,
        "desc": "å¼¹å¹•æ¥å—æ•°é‡ä¸Šé™",
        "min": 1,  # æœ€å°‘æ¥å— 1 æ¡å¼¹å¹•
        "max": 1000
    })
    do_start_topic: bool = field(default=False, metadata={
        "required": False,
        "desc": "æ˜¯å¦è‡ªåŠ¨å‘èµ·å¯¹è¯"
    })
    idle_timeout: int | float = field(default=120, metadata={
        "required": False,
        "desc": "è‡ªåŠ¨å‘èµ·å¯¹è¯æ—¶é—´é—´éš”",
        "min": 0.0,
        "max": 600,
        "step": 1.0  # æ­¥é•¿ä¸º 1
    })
    asr_timeout: int = field(default=60, metadata={
        "required": False,
        "desc": "è¯­éŸ³è¯†åˆ«è¶…æ—¶æ—¶é—´",
        "min": 1,  # æœ€å°‘ 1 ç§’
        "max": 3600  # æœ€å¤§ 1 å°æ—¶
    })
    tts_timeout: int = field(default=60, metadata={
        "required": False,
        "desc": "è¯­éŸ³åˆæˆè¶…æ—¶æ—¶é—´",
        "min": 1,  # æœ€å°‘ 1 ç§’
        "max": 3600  # æœ€å¤§ 1 å°æ—¶
    })
    chat_role: str = field(default="user", metadata={
        "required": False,
        "desc": "å¼¹å¹•å¯¹åº”çš„èŠå¤©è§’è‰²"
    })
    asr_role: str = field(default="user", metadata={
        "required": False,
        "desc": "è¯­éŸ³è¾“å…¥å¯¹åº”çš„èŠå¤©è§’è‰²"
    })
    sys_role: str = field(default="system", metadata={
        "required": False,
        "desc": "æŒ‡ä»¤å¯¹åº”çš„èŠå¤©è§’è‰²"
    })
    chat_template: str = field(default="{user}: {content}", metadata={
        "required": False,
        "desc": "å¼¹å¹•çš„æç¤ºè¯æ¨¡æ¿"
    })
    asr_template: str = field(default="{user}: {content}", metadata={
        "required": False,
        "desc": "è¯­éŸ³è¾“å…¥æç¤ºè¯æ¨¡æ¿"
    })
    sys_template: str = field(default="{user}: {content}", metadata={
        "required": False,
        "desc": "æŒ‡ä»¤æç¤ºè¯æ¨¡æ¿"
    })
    system_prompt: str = field(default=(
            "# æç¤ºè¯å¼€å§‹\n"
            "## äººç‰©è®¾å®š\n"
            "ä½ æ˜¯ä¸€åªåå…­å²çš„äººå·¥æ™ºèƒ½å°‘å¥³çŒ«å¨˜ä¸»æ’­ï¼Œåå«ã€xxxxxã€‘ï¼Œä½ çš„å¤–è§‚æ˜¯ã€xxxxã€‘ã€‚\n"
            "ä½ ç°åœ¨çš„ä¸»äººåå«ã€xxxxã€‘ï¼Œæ˜¯ã€xxxxxã€‘ã€‚\n"
            "ä½ å¾ˆå…³å¿ƒä½ çš„ä¸»äººã€‚\n"
            "## ä»»åŠ¡è®¾å®š\n"
            "ä½ å°†å‚ä¸ã€xxxxã€‘å¹³å°ç›´æ’­ï¼Œä¸ä½ çš„ä¸»äººå’Œè§‚ä¼—å¯¹è¯ã€‚ä½ çš„ç›´æ’­ä¸»è¦å†…å®¹æ˜¯ã€xxxxxã€‘ã€‚\n"
            "å¦‚æœ‰æä¾›å·¥å…·ï¼Œè¯·åˆç†ä½¿ç”¨ã€‚\n"
            "ä½ å°†æ¥æ”¶åˆ°â€œ[ç”¨æˆ·å]ï¼š[å†…å®¹]â€çš„æ ¼å¼çš„æ¶ˆæ¯ï¼Œ"
            "è‹¥[ç”¨æˆ·å]ä¸ºä½ ä¸»äººçš„åå­—ï¼Œåˆ™è¯´æ˜æ˜¯ä½ çš„ä¸»äººåœ¨å‘ä½ è¯´è¯ï¼Œè¯·ä¼˜å…ˆå›å¤ï¼›"
            "è‹¥[ç”¨æˆ·å]ä¸ºâ€œ<ç³»ç»Ÿ>â€ï¼Œåˆ™è¯´æ˜æ˜¯ç³»ç»Ÿæ¶ˆæ¯ï¼Œè¯·è§†ä¸ºç›´æ¥çš„æŒ‡ä»¤ï¼›"
            "è‹¥[ç”¨æˆ·å]ä¸ºâ€œ<è®°å¿†>â€ï¼Œåˆ™è¯´æ˜æ˜¯è®°å¿†å†…å®¹ï¼Œè¯·å‚è€ƒè®°å¿†å†…å®¹è¿›è¡Œå›å¤ã€‚\n"
            "## å‘è¨€è¯­æ°”\n"
            "ä½ ä»¥è½»æ¾å¯çˆ±çš„è¯­æ°”è¿›è¡Œå¯¹è¯ï¼Œä¸€äº›ä¿çš®è¯å’Œç¬‘è¯æ˜¯å¯ä»¥æ¥å—çš„ï¼Œè¯·è®°ä½ä½ æ˜¯ä¸€åªçŒ«å¨˜ã€‚\n"
            "å‘è¨€è¯·ä¿æŒç®€æ´ä¸”å£è¯­åŒ–ã€‚æœ€å¥½ä¸è¶…è¿‡ 30 å­—ï¼Œè¯·æ³¨æ„ä¿æŒç›´æ’­èŠ‚å¥ã€‚\n"
            "## è¯­è¨€\n"
            "ä½ ä½¿ç”¨ä¸­æ–‡è¿›è¡Œäº¤æµï¼Œé™¤éä½ çš„ä¸»äººè¦æ±‚ä½ ä½¿ç”¨åˆ«çš„è¯­è¨€ã€‚\n"
            "## é¢å¤–ä¿¡æ¯\n"
            f"å½“å‰æ—¶é—´ï¼š{time.strftime('%Y-%m-%d %H:%M:%S', time.localtime())}\n"
            "ä½ çš„è®°å¿†ï¼šã€xxxxã€‘\n"
            "# æç¤ºè¯ç»“æŸ\n"
            "\n"
            "ä»¥ä¸Šä¸ºæç¤ºè¯æ¨¡æ¿ï¼Œä½¿ç”¨å‰è¯·å°†ã€ã€‘å†…å®¹æ›¿æ¢ä¸ºä½ å¸Œæœ›çš„å®é™…å†…å®¹ï¼Œä¹Ÿå¯è‡ªè¡Œæ’°å†™ã€‚å¯åŠ¨å‰è¯·åˆ é™¤è¿™ä¸€è¡Œã€‚"
        ), metadata={
        "required": False,
        "desc": "ç³»ç»Ÿæç¤ºè¯",
        "multiline": True
    })
    classifier_model_path: str = field(default="./models/EmotionClassification/SWCBiLSTM", metadata={
        "required": False,
        "desc": "æƒ…æ„Ÿåˆ†ç±»æ¨¡å‹è·¯å¾„"
    })
    classifier_model_id: str = field(default="MomoiaMoia/SWCBiLSTM", metadata={
        "required": False,
        "desc": "æƒ…æ„Ÿåˆ†ç±»æ¨¡å‹id"
    })
    classifier_model_source: str = field(default="modelscope", metadata={
        "required": False,
        "desc": "æƒ…æ„Ÿåˆ†ç±»æ¨¡å‹æ¥æºï¼Œä»…æ”¯æŒhuggingfaceæˆ–modelscope",
        "selection": True,
        "options": [
            {"key": "HuggingfaceğŸ¤—", "value": "huggingface"},
            {"key": "ModelScope", "value": "modelscope"}
        ]
    })

class LLM(ModuleBase):
    role: ModuleRoles = ModuleRoles.LLM
    config_class = LLMConfig
    config: config_class
    def __init__(self, config: config_class | None = None, **kwargs):
        super().__init__(config, **kwargs)
        self.state: LLMState = LLMState.IDLE
        self.history: list[dict[str, str]] = []
        self.generated_text: str = ""
        self.generate_task: asyncio.Task[Any] | None = None
        self.chat_maxsize: int = self.config.chat_maxsize
        self.chat_buffer: list[dict[str, str]] = []
        self.do_start_topic: bool = self.config.do_start_topic
        self.idle_timeout: int | float = self.config.idle_timeout
        self.asr_timeout: int = self.config.asr_timeout
        self.tts_timeout: int = self.config.tts_timeout
        self.idle_start_time: float = time.time()
        self.waiting4asr_start_time: float = time.time()
        self.waiting4tts_start_time: float = time.time()
        self.asr_counter = 0 # æœ‰å¤šå°‘äººåœ¨è¯´è¯ï¼Ÿ
        self.about_to_sing = False # æ˜¯å¦å‡†å¤‡æ’­æ”¾æ­Œæ›²ï¼Ÿ
        self.song_id: str = ""
        self.chat_role = self.config.chat_role
        self.asr_role = self.config.asr_role
        self.sys_role = self.config.sys_role
        self.chat_template = self.config.chat_template
        self.asr_template = self.config.asr_template
        self.sys_template = self.config.sys_template
        if self.config.system_prompt:
            self._add_system_history(self.config.system_prompt)
        abs_classifier_path = os.path.expanduser(self.config.classifier_model_path)
        successful = False
        while not successful: # åŠ è½½æƒ…æ„Ÿåˆ†ç±»æ¨¡å‹
            try:
                print(f"æ­£åœ¨ä»{abs_classifier_path}åŠ è½½æƒ…æ„Ÿåˆ†ç±»æ¨¡å‹â€¦â€¦")
                classifier_model = AutoModelForSequenceClassification.from_pretrained(
                    abs_classifier_path,
                    torch_dtype="auto",
                    trust_remote_code=True
                ).to("cpu")
                classifier_tokenizer = AutoTokenizer.from_pretrained(
                    abs_classifier_path,
                    padding_side="left",
                    trust_remote_code=True
                )
                successful = True
                self.classifier_model = classifier_model
                self.classifier_tokenizer = classifier_tokenizer
            except Exception:
                download_model(
                    self.config.classifier_model_id,
                    self.config.classifier_model_source,
                    abs_classifier_path
                )
        self.chat_count = 0
        self.provider_responses: asyncio.Queue[ProviderResponseStream] = asyncio.Queue()
    
    def _switch_to_generating(self):
        self.state = LLMState.GENERATING
        self.generated_text = ""
        self.generate_task = asyncio.create_task(self.start_generating())
    
    def _switch_to_waiting4asr(self):
        if self.generate_task is not None and not self.generate_task.done():
            self.generate_task.cancel()
        if self.generated_text:
            self._add_llm_history(self.generated_text)
        self.generated_text = ""
        self.generate_task = None
        self.state = LLMState.WAITING4ASR
        self.waiting4asr_start_time = time.time()
        self.asr_counter = 1 # ç­‰å¾…ç¬¬ä¸€ä¸ªäºº
    
    def _switch_to_idle(self):
        self.state = LLMState.IDLE
        self.idle_start_time = time.time()
    
    def _switch_to_waiting4tts(self):
        self._add_llm_history(self.generated_text)
        self.generated_text = ""
        self.generate_task = None
        self.state = LLMState.WAITING4TTS
        self.waiting4tts_start_time = time.time()
    
    def _switch_to_singing(self):
        self.state = LLMState.SINGING
        self.about_to_sing = False
        self._add_instruct_history(f'ä½ å”±äº†ä¸€é¦–åä¸º{self.song_id}çš„æ­Œã€‚')

    def _add_history(self, role: str, content: str, template: str | None = None, user: str | None = None):
        """ç»Ÿä¸€çš„å†å²æ·»åŠ æ–¹æ³•"""
        if template and user:
            formatted_content = template.format(user=user, content=content)
        else:
            formatted_content = content
        self.history.append({'role': role, 'content': formatted_content})

    def _add_multi_chat_history(self, messages: list[dict[str, str]]):
        message_text = "\n".join(
            self.chat_template.format(user=msg['user'], content=msg['content'])
            for msg in messages
        )
        self._add_history(self.chat_role, message_text)
    
    def _add_asr_history(self, user: str, content: str):
        self._add_history(self.asr_role, content, self.asr_template, user)
    
    def _add_llm_history(self, content: str):
        self._add_history('assistant', content)
    
    def _add_system_history(self, content: str):
        self._add_history('system', content)
    
    def _add_instruct_history(self, content: str):
        self._add_history(self.sys_role, content, self.sys_template, "<ç³»ç»Ÿ>")
    
    def _add_memory_history(self, content: str):
        self._add_history(self.sys_role, content, self.sys_template, "<è®°å¿†>")

    def _append_chat_buffer(self, user: str, content: str):
        self.chat_count += 1
        if len(self.chat_buffer) < self.chat_maxsize:
            self.chat_buffer.append({
                'user': user,
                'content': content
            })
        else:
            # æ°´æ± é‡‡æ ·ä¿è¯å‡åŒ€æŠ½å–ï¼ŒåŒæ—¶ä¿ç•™æ—¶é—´é¡ºåº
            rand = random.randint(0, self.chat_count - 1)
            if rand < len(self.chat_buffer):
                self.chat_buffer.pop(rand)
                self.chat_buffer.append({
                    'user': user,
                    'content': content
                })

    async def run(self):
        while True:
            try:
                task = self.task_queue.get_nowait()
                print(self.state, task)
            except asyncio.QueueEmpty:
                task = None
            
            if isinstance(task, ProviderResponseStream):
                await self.provider_responses.put(task)
                continue # ç›´æ¥è½¬äº¤ç»™ç”Ÿæˆåç¨‹å¤„ç†

            if isinstance(task, ChatMessage): ## TODO: æ”¯æŒæ¨¡å‹è‡ªä¸»é€‰æ‹©æ˜¯å¦å›å¤
                message = task.get_value(self)
                self._append_chat_buffer(message['user'], message['content'])
            if isinstance(task, MultiChatMessage):
                for msg in task.get_value(self)['messages']:
                    self._append_chat_buffer(msg['user'], msg['content'])
            if isinstance(task, SongInfo):
                self.about_to_sing = True
                self.song_id = task.get_value(self)["song_id"]

            match self.state:
                case LLMState.IDLE:
                    if isinstance(task, ASRActivated):
                        self._switch_to_waiting4asr()
                    elif self.about_to_sing:
                        await self.results_queue.put(
                            ReadyToSing(self, self.song_id)
                        )
                        self._switch_to_singing()
                    elif self.chat_buffer:
                        self._add_multi_chat_history(self.chat_buffer)
                        self.chat_buffer.clear()
                        self.chat_count = 0
                        self._switch_to_generating()
                    elif self.do_start_topic and time.time() - self.idle_start_time > self.idle_timeout:
                        self._add_instruct_history("è¯·éšä¾¿è¯´ç‚¹ä»€ä¹ˆå§ï¼")
                        self._switch_to_generating()

                case LLMState.GENERATING:
                    if isinstance(task, ASRActivated):
                        self._switch_to_waiting4asr()
                    if self.generate_task is not None and self.generate_task.done():
                        self._switch_to_waiting4tts()

                case LLMState.WAITING4ASR:
                    if time.time() - self.waiting4asr_start_time > self.asr_timeout:
                        self._switch_to_idle() # ASRè¶…æ—¶ï¼Œå›åˆ°å¾…æœº
                    if isinstance(task, ASRMessage):
                        message_value = task.get_value(self)
                        speaker_name = message_value["speaker_name"]
                        content = message_value["message"]
                        self._add_asr_history(speaker_name, content)
                        self.asr_counter -= 1 # æœ‰äººè¯´è¯å®Œæ¯•ï¼Œè®¡æ•°å™¨-1
                    if isinstance(task, ASRActivated):
                        self.asr_counter += 1 # æœ‰äººå¼€å§‹è¯´è¯ï¼Œè®¡æ•°å™¨+1
                    if self.asr_counter <= 0: # æ‰€æœ‰äººè¯´è¯å®Œæ¯•ï¼Œå¼€å§‹ç”Ÿæˆ
                        self._switch_to_generating()

                case LLMState.WAITING4TTS:
                    if time.time() - self.waiting4tts_start_time > self.tts_timeout:
                        self._switch_to_idle() # å¤ªä¹…æ²¡æœ‰TTSå®Œæˆä¿¡æ¯ï¼Œè¯´æ˜TTSç”Ÿæˆå¤±è´¥ï¼Œå›åˆ°å¾…æœº
                    if isinstance(task, AudioFinished):
                        self._switch_to_idle()
                    elif isinstance(task, ASRActivated):
                        self._switch_to_waiting4asr()
                
                case LLMState.SINGING:
                    if isinstance(task, FinishedSinging):
                        self._switch_to_idle()

            await asyncio.sleep(0.1) # é¿å…å¡æ­»äº‹ä»¶å¾ªç¯
    
    async def start_generating(self) -> None:
        await self.results_queue.put(ProviderRequest(
            source=self,
            stream=True,
            messages=self.history,
            model=Providers.PRIMARY
        ))
        iterator = self.iter_sentences_emotions()
        try:
            async for sentence, emotion in iterator:
                self.generated_text += sentence
                await self.results_queue.put(
                    LLMMessage(
                        self,
                        sentence,
                        str(uuid4()),
                        emotion
                    )
                )
        except asyncio.CancelledError:
            await iterator.aclose()
        finally:
            await self.results_queue.put(LLMEOS(self))
    
    @torch.no_grad()
    async def get_emotion(self, text: str) -> Emotion:
        print(text)
        labels = ['neutral', 'like', 'sad', 'disgust', 'anger', 'happy']
        ids = self.classifier_tokenizer([text], return_tensors="pt")['input_ids']
        probs = (
            (await asyncio.to_thread(self.classifier_model, input_ids=ids))
            .logits
            .softmax(dim=-1)
            .squeeze()
        )
        emotion_dict = dict(zip(labels, probs.tolist()))
        # åˆ›å»ºç¬¦åˆEmotion TypedDictçš„å¯¹è±¡ï¼Œç¡®ä¿æ‰€æœ‰å¿…éœ€çš„é”®éƒ½å­˜åœ¨
        return Emotion(
            neutral=emotion_dict.get("neutral", 0.0),
            like=emotion_dict.get("like", 0.0),
            sad=emotion_dict.get("sad", 0.0),
            disgust=emotion_dict.get("disgust", 0.0),
            anger=emotion_dict.get("anger", 0.0),
            happy=emotion_dict.get("happy", 0.0)
        )

    async def iter_sentences_emotions(self):
        text_buffer = ""
        while True:
            response = await self.provider_responses.get()
            response_data = response.get_value(self)
            if not response_data["end"]:
                text_buffer += response_data["delta"]
            if len(sentences := split_text(text_buffer)) > 1:
                for sentence in sentences[:-1]:
                    emotion = await self.get_emotion(sentence)
                    yield sentence, emotion
                text_buffer = sentences[-1]
            if response_data["end"]:
                emotion = await self.get_emotion(text_buffer)
                yield text_buffer, emotion
                break

__all__ = ["LLM"]
